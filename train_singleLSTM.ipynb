{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "Using TensorFlow backend.\n"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from math import cos, sin, atan2, sqrt, pi, radians, degrees, asin\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parse(x):\n",
    "    x = x[:4] + ' ' + x[4:6] + ' '+x[6:]\n",
    "    return datetime.strptime(x, '%Y %m %d')\n",
    "def read_data(file):\n",
    "    return pd.read_csv(file, parse_dates=['date'], date_parser=parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A = read_data('data/A.csv')\n",
    "B = read_data('data/B.csv')\n",
    "C = read_data('data/C.csv')\n",
    "D = read_data('data/D.csv')\n",
    "E = read_data('data/E.csv')\n",
    "n_features = A.shape[1] - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "n_step = 1\n",
    "n_ob = n_step * n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "    n_vars = 1 if type(data) is list else data.shape[1]\n",
    "    df = pd.DataFrame(data)\n",
    "    cols, names = list(), list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "    for i in range(n_in, 0, -1):\n",
    "        cols.append(df.shift(i))\n",
    "        names += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # forecast sequence (t, t+1, ... t+n)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "        if i == 0:\n",
    "            names += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
    "        else:\n",
    "            names += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
    "    # put it all together\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    agg.columns = names\n",
    "    # drop rows with NaN values\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    return agg\n",
    "\n",
    "def process(cdf):\n",
    "    v = []\n",
    "    for i in range(cdf.region.max()+1):\n",
    "        df = cdf[cdf.region == i].copy()\n",
    "        df = df.set_index(\"date\")\n",
    "        df.drop([\"city\", \"region\"], axis=1, inplace=True)\n",
    "        values = df.values\n",
    "        values = values.astype('float32')\n",
    "        # scaled = scaler.fit_transform(values)\n",
    "        reframed = series_to_supervised(values, n_step, 1)\n",
    "        # reframed.drop(reframed.columns[range(53, 104)], axis=1, inplace=True)\n",
    "        v.append(reframed.values)\n",
    "        a = v[0]\n",
    "        for i in range(1, len(v)):\n",
    "            a = np.concatenate((a, v[i]), axis=0)\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A1 = read_data('data/A1.csv')\n",
    "a = process(A)\n",
    "b = process(B)\n",
    "c = process(C)\n",
    "d = process(D)\n",
    "e = process(E)\n",
    "data = np.concatenate((a,b,c,d,e),axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[:, :n_ob], data[:, n_ob:], test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], n_step, n_features))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], n_step, n_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n\nWARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n\nWARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n\nWARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1702: The name tf.log is deprecated. Please use tf.math.log instead.\n\nWARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nUse tf.where in 2.0, which has the same broadcast rule as np.where\nWARNING:tensorflow:From /anaconda3/envs/tensorflow/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n\nTrain on 13798 samples, validate on 3450 samples\nEpoch 1/4\n - 3s - loss: 0.0419 - val_loss: 0.0272\nEpoch 2/4\n - 3s - loss: 0.0201 - val_loss: 0.0196\nEpoch 3/4\n - 2s - loss: 0.0165 - val_loss: 0.0158\nEpoch 4/4\n - 2s - loss: 0.0141 - val_loss: 0.0131\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1a3b4f3b70>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(units=128, input_shape=(X_test.shape[1], X_test.shape[2])))\n",
    "# model.add(LSTM(units=52))\n",
    "model.add(Dense(n_features))\n",
    "# model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='msle', optimizer='adam')\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=16, validation_data=(X_test, y_test), verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "yhat = model.predict(X_test)\n",
    "# print(yhat)\n",
    "# x_test0 = X_test.reshape((X_test.shape[0], n_features*n_step))\n",
    "# inv_yhat = np.concatenate((yhat, x_test0[:, n_ob:]), axis=1)\n",
    "# inv_yhat = scaler.inverse_transform(yhat)\n",
    "# inv_yhat = inv_yhat[:, 0]\n",
    "# y_test = y_test.reshape((len(y_test), n_features))\n",
    "# inv_y = np.concatenate((y_test, x_test0[:, n_ob:]), axis=1)\n",
    "# inv_y = scaler.inverse_transform(y_test)\n",
    "# inv_y = inv_y[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Test RMSE: 1.164\n"
    }
   ],
   "source": [
    "# RMSE\n",
    "rmse = sqrt(mean_squared_error(y_test, yhat))\n",
    "print('Test RMSE: %.3f' % rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_last_day(cdf):\n",
    "    v = []\n",
    "    for i in range(cdf.region.max()+1):\n",
    "        df = cdf[cdf.region == i].copy()\n",
    "        df = df.set_index(\"date\")\n",
    "        df.drop([\"city\", \"region\"], axis=1, inplace=True)\n",
    "        values = df.values\n",
    "        values = values.astype('float32')\n",
    "        # scaled = scaler.fit_transform(values)\n",
    "        reframed = series_to_supervised(values, n_step, 1)\n",
    "        v.append(np.array([reframed.values[-1]])[:,n_ob:])\n",
    "    return v\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_lastday = process_last_day(A)\n",
    "b_lastday = process_last_day(B)\n",
    "c_lastday = process_last_day(C)\n",
    "d_lastday = process_last_day(D)\n",
    "e_lastday = process_last_day(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict(day_before):\n",
    "    day_before = np.reshape(day_before, (day_before.shape[0], n_step, n_features))\n",
    "    day_after = model.predict(day_before)\n",
    "    return day_after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_future_prediction(lastday):\n",
    "    res_city_lst = []\n",
    "    for region_begin in lastday:\n",
    "        print(region_begin)\n",
    "        res_date_lst = [predict(region_begin)]\n",
    "        for i in range(1, 30):\n",
    "            res_date_lst.append(predict(res_date_lst[-1]))\n",
    "        res_city_lst.append(res_date_lst)\n",
    "    return res_city_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "  0.6136364  0.57894737 0.6153846  0.40000004 0.40625    0.6875\n  0.26388887 0.55       0.3205128  0.6666666  0.20270273 0.6500001\n  0.23943666 0.6363636  0.28767124 0.6        0.2837838  0.7307693\n  0.42307696 0.5555556  0.5555556  0.6956522  0.49999997 0.39130434\n  0.34545457 0.42857143 0.38297868 0.375      0.39999992 0.47368422\n  0.39999992 0.35413164 0.13644001 0.19092464]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.        ]]\n[[0.8333334  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.24235749]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.90598124]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.05695421]]\n[[0.99999994 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.0143411 ]]\n[[0.74358976 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.96487117]]\n[[0.7657658  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.60694885]]\n[[0.99999994 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.15904903]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.00840536]]\n[[0.6818182  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.9585875 ]]\n[[0.9298246  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.96438015]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.8588712 ]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.10639578]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.9616772 ]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.17176521]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.02541423]]\n[[0.99999994 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.47383952]]\n[[0.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.90166414]]\n[[0.33333334 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.20584214]]\n[[0.33333334 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.94532716]]\n[[0.9302325  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.02946234]]\n[[0.47058824 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.9385988 ]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.00875676]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.06396914]]\n[[0.99999994 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.96567845]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.33962345]]\n[[0.8571429  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.11606288]]\n[[0.5714286  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 1.        ]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.11557317]]\n[[0.40000004 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.66491795]]\n[[0.95348835 0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.24738622]]\n[[0.6756757  0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.04693693]]\n[[1.         0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.9400085 ]]\n[[0.5        0.3809524  0.85416675 0.25       0.8333333  0.33333334\n  0.6363636  0.2777778  0.8292682  0.2631579  0.88       0.21739131\n  0.89130425 0.86956525 0.89130425 0.87999994 0.86111104 0.30434784\n  0.5405406  0.36842105 0.48936164 0.5        0.39215696 0.5882354\n  0.0625     0.6        0.24999994 0.631579   0.40740743 0.54545456\n  0.37878782 0.65       0.265625   0.5        0.55714285 0.28\n  0.5294117  0.6818182  0.6571429  0.33333334 0.61016935 0.7\n  0.70491797 0.5        0.6666666  0.61904764 0.7173914  0.3809524\n  0.74509805 0.4489988  0.18221802 0.7213033 ]]\n"
    }
   ],
   "source": [
    "a_prediction = generate_future_prediction(a_lastday)\n",
    "b_prediction = generate_future_prediction(b_lastday)\n",
    "c_prediction = generate_future_prediction(c_lastday)\n",
    "d_prediction = generate_future_prediction(d_lastday)\n",
    "e_prediction = generate_future_prediction(e_lastday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 1.5134339 ,  3.262701  ,  0.70592916,  0.9038207 ,  0.76391673,\n         0.99305165,  0.7706631 , -4.541298  ,  0.75200313,  1.8693546 ,\n         0.8222115 ,  1.0580941 ,  0.8169484 ,  4.7765985 ,  0.81093776,\n         1.4679995 ,  0.8648067 ,  7.608754  ,  0.8363721 ,  8.55347   ,\n         0.6103633 ,  9.390717  ,  0.541253  , 13.050402  ,  0.634488  ,\n        12.729559  ,  0.5326317 , 13.288866  ,  0.3798835 , 15.15965   ,\n         0.40002948, 14.240046  ,  0.36940652, 14.895222  ,  0.3931324 ,\n         6.6268206 ,  0.36837894,  6.8567505 ,  0.42582583,  7.947989  ,\n         0.5676015 ,  7.029104  ,  0.5507684 ,  6.464271  ,  0.7619359 ,\n         6.417109  ,  0.761809  ,  4.232056  ,  0.75528395,  0.22261022,\n         0.21896641,  2.1021855 ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "a_prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_res_infection_lst(cities_prediction):\n",
    "    res_infection = []\n",
    "    res_region = []\n",
    "    for city_prediction in cities_prediction:\n",
    "        region_id = 0\n",
    "        for region in city_prediction:\n",
    "            for date in region:\n",
    "                res_infection.append(int(round(date[0][0])))\n",
    "                res_region.append(region_id)\n",
    "            region_id = region_id + 1\n",
    "    return res_infection, res_region\n",
    "def check_minus(infection):\n",
    "    for i in range(len(infection)):\n",
    "        if infection[i] < 0:\n",
    "            if infection[i - 1] < 6:\n",
    "                infection[i] = 0\n",
    "            else:\n",
    "                infection[i] = int(infection[i - 1])\n",
    "    return infection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "infection, region = generate_res_infection_lst([a_prediction,b_prediction,c_prediction,d_prediction,e_prediction])\n",
    "infection = check_minus(infection)\n",
    "submission = pd.read_csv('data/submission.csv', header=None, names=['city','region','date','infection'])\n",
    "submission['infection'] = infection\n",
    "submission['region'] = region\n",
    "submission.to_csv('data/test_submission_single_LSTM.csv', header=None, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "result  \n",
    "100 - 97.669  \n",
    "80 - 98.666  \n",
    "70 - 98.581    \n",
    "50 - 99.553  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit ('tensorflow': conda)",
   "language": "python",
   "name": "python36964bittensorflowconda0768426cf85c4c639c5d84af9151c906"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}